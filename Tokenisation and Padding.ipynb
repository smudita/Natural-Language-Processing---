{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow==2.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'i love my dog',\n",
    "    'I, love my cat',\n",
    "    'You love my dog!',\n",
    "    'Do you actually love my dog too?'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 1, 'my': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'actually': 8, 'too': 9}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words = 100)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 1, 2, 3], [4, 1, 2, 6], [5, 1, 2, 3], [7, 5, 8, 1, 2, 3, 9]]\n"
     ]
    }
   ],
   "source": [
    "print(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text to sequences called can take any set of sentences, so it can encode them based on the word set that it learned from the one that was passed into fit on texts(i.e.sentences).\n",
    "If you train a neural network on a corpus of texts, and the text has a word index generated from it, then when you want to do inference with the train model(sentences), you'll have to encode the text that you want to infer on with the same word index, otherwise it would be meaningless. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    'i really love my dog',\n",
    "    'my dog loves my manatee'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 1, 2, 3], [2, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "test_seq=tokenizer.texts_to_sequences(test_data)\n",
    "print(test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Index =  {'love': 1, 'my': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'actually': 8, 'too': 9}\n",
      "\n",
      "Sequences =  [[4, 1, 2, 3], [4, 1, 2, 6], [5, 1, 2, 3], [7, 5, 8, 1, 2, 3, 9]]\n",
      "\n",
      "Padded Sequences:\n",
      "[[0 4 1 2 3]\n",
      " [0 4 1 2 6]\n",
      " [0 5 1 2 3]\n",
      " [8 1 2 3 9]]\n"
     ]
    }
   ],
   "source": [
    "padded = pad_sequences(sequences, maxlen=5)\n",
    "print(\"\\nWord Index = \" , word_index)\n",
    "print(\"\\nSequences = \" , sequences)\n",
    "print(\"\\nPadded Sequences:\")\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padded Test Sequence: \n",
      "[[0 0 0 0 0 0 4 1 2 3]\n",
      " [0 0 0 0 0 0 0 2 3 2]]\n"
     ]
    }
   ],
   "source": [
    "padded = pad_sequences(test_seq, maxlen=10)\n",
    "print(\"\\nPadded Test Sequence: \")\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")  #For the unknown words in corpus we define this(out of vocabulary)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'love': 2, 'my': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'actually': 9, 'too': 10}\n"
     ]
    }
   ],
   "source": [
    "print(word_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 1, 2, 3, 4], [3, 4, 1, 3, 1]]\n"
     ]
    }
   ],
   "source": [
    "test_seq=tokenizer.texts_to_sequences(test_data)\n",
    "print(test_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding padding in the end. By default padding is from the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padded Test Sequence: \n",
      "[[5 1 2 3 4 0 0 0 0 0]\n",
      " [3 4 1 3 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "padded = pad_sequences(test_seq, maxlen=10,padding='post')\n",
    "print(\"\\nPadded Test Sequence: \") \n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
